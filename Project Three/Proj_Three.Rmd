---
title: "Project Three"
author: "John K. Hancock"
date: "September 24, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```

#PROJECT THREE: Data Science Skills

####by The Project Three Team:
####Joshua Bentley
####Chester Poon
####John K. Hancock

##Project Objective: Use data analysis to answer the question: "Which are the most valued data science skills?

####I. Introduction:

To answer that question, the Project Three team first met on September 13th via Google hangout to discuss an approach to the project.  In order to answer the question about highly valued data science skills, we had to go where there was data, and the most likely places were job search sites which would provide the data through job postings. At our meeting, We decided to research web APIs for the most popular job search websites, CareerBuilder, LinkedIn, Glassdoor, Monster, Indeed, Angelist, Upwork, Idealist, Hired, Dice, Fiverr, Zip Recruiter, Kaggle and others. 

A week later, we came up empty in our search for APIs. Most of the sites restricted access to their API or require a partnership with the site. We then turned our attention to web-scraping the sites.  Each one of us selected a site, Chester Poon (LinkedIn), Joshua Bentley(CareerBuilder), and John Hancock(Indeed) where we attempted to scrape the data. Web scraping proved very challenging for us. In the sections below, we each recount our experience with scraping each site.

**John Hancock (Indeed.com)**:

The Indeed.com job site consisted of running a search for jobs and returing a page of hyperlinks for each job where you can find a full description of the job. I discovered that the R package, rvest, does not work well with web pages where you have to click on a link to access the information as the Indeed.com site had.  So, I turned to Python to do the scraping.  In particular, I used the Selenium package which uses a webdriver that automates a web browser to access websites.

My process was automating a search for Data Science jobs in New York, NY.  The code navigates through the over 900 pages returned from the search. As stated earlier, each search results page contained hyperlinks to each job where contained more detailed information.  The Python code identifies each job link by their XPath variable, puts all of the links on the page into a list, iterates over that list and clicks on each link. Once the page was open, the job description, title, company were scraped from the page.  The entire process took quite a while.  It ran for more than three days, but I was able to get 330 job postings. 


(Python Code)[https://github.com/JohnKHancock/DATA-607-Data-Acquisition-and-Management/blob/master/Project%20Three/webscraper2.html]
(See the Python notebook, webscraper2.ipynb)


**Chester Poon (LinkedIn.com)**:

Challenges and Next steps for scraping LinkedIn

One of the major challenges that was specific to LinkedIn was the website's security barriers. We were only able to do a one time scrape before the site flagged my profile and IP address as the source of a bot. I was able to get the initial pull data, however I was interested in pulling more data to be better in line with my teammates data. I was unable to do so because as soon as I would initialize the script, LinkedIn would automatically log me out. I had also tried creating a fake account as well as using Joshua's profile as a way to get around security without any luck. It was actually here where we discovered that for each login and for each person accessing the site, there were slight differences in how the page rendered that confounded the script. For example, one version had the text of a button tag nested inside a span tag whereas the other version did not. Other times, the attribute id was different in one version of the site versus the other. These dynamically changing sites and the action of logging me out when they detected a bot, limited our scope.

As a next step, it would be best to gain permission from LinkedIn to webscrape their site. It was discovered rather late in the process that websites might have a `robots.txt` that specifies rules for webscraping as well as a contact email on how to gain access to scraping. Knowing this sooner could have saved much time. LinkedIn's `robots.txt` file is located here: <https://www.linkedin.com/robots.txt>



**Joshua Bentley (CareerBuilder.com)**:




##II. Data Collection and Compilation into Final DataFrame

This section details how the collected data was compiled into a final data frame.

```{r, include=FALSE}
library(tidyverse)  
library(rvest)
library(stringr)
library(dplyr)
library(ggplot2)
library(rebus)
library(lubridate)
library(tm)
library(purrr)
library(plotly)
library(wordcloud)
library(DBI)
library(RSQLite)
```

##LinkedIn Data Science Jobs

###Data webscraped by Chester Poon.  

####The data that Chester scraped from LinkedIn was more structured than the data that Josh and I were able to scrape from CareerBuilder and Indeed. For most of the postings on LinkedIn, job qualifications were broken out in its own section.  The csv that Chester delivered was used as the basis for identifying skills scraped from the other two websites. 

```{r}
LinkedIn_DSjobs <- read.csv("LinkedIn.csv", stringsAsFactors = FALSE)

#Cleaning the phrase "Programming Language" from Python so that we can match it in data from the other websites.
LinkedIn_DSjobs$skills<-str_trim(str_replace(LinkedIn_DSjobs$skills, "\\(Programming Language\\)", ""))

```

```{r}

frequency<- count(LinkedIn_DSjobs, skills)
top_Skills<- frequency[frequency$n > 0,]
top_Skills[order(-top_Skills$n),] 
```


##Indeed Data Science Jobs

###Data webscraped by John K Hancock. 

####The scraped data from both Indeed.com and CareerBuilder.com was much less structured. The job postings were more free-form text than structured like LinkedIn.  Skills requested were not broken out into their section.  The entire text from the job description along with the Job Title and Job Company were scraped into csv files.



```{r, options(warn=-1)}
Indeed_DSjobs <- read.csv("Data_Science_Indeed_Jobs.csv", stringsAsFactors = FALSE, encoding = "UTF-8")
Indeed_DSjobs$Description <- str_replace_all(Indeed_DSjobs$Description,"[\n]","")
top_Skills_Indeed <- data.frame(skills=character(), n=integer())

```

####Each skill listed from LinkedIn were searched in the description of the job, and the number of hits were captured in a variable, "n". 

```{r}
for (i in 1:nrow(top_Skills))
{
     newRow <- data.frame(skills=top_Skills$skills[i], 
                          n=length(unlist(str_extract_all(Indeed_DSjobs$Description,top_Skills$skills[i]))))
    top_Skills_Indeed <- rbind(top_Skills_Indeed, newRow)

}
top_Skills_Indeed$skills <- as.character(top_Skills_Indeed$skills)
top_Skills_Indeed<-top_Skills_Indeed[!duplicated(top_Skills_Indeed),]
```

####In the end, we get a data frame which lists each skill and the number of times that skill is mentioned in the job descriptions on Indeed.com

```{r}
top_Skills_Indeed[order(-top_Skills_Indeed$n),]
```


##CareerBuilder Data Science Jobs

###Data webscraped by Joshua Bentley. 

####The process for Indeed.com was repeated for CareerBuilder.com

```{r}
CareerBuilder_DSjobs <- read.csv("careerbuilder.csv", encoding = "UTF-8")
CareerBuilder_DSjobs$jobdesc <- str_replace_all(CareerBuilder_DSjobs$jobdesc,"[\n]","")
top_Skills_CB <- data.frame(skills=character(), n=integer())

```

```{r}
for (i in 1:nrow(top_Skills))
{
     newRow <- data.frame(skills=top_Skills$skills[i], 
                          n=length(unlist(str_extract_all(CareerBuilder_DSjobs$jobdesc,top_Skills$skills[i]))))
    top_Skills_CB<- rbind(top_Skills_CB, newRow)

}
```


```{r}
top_Skills_CB<-top_Skills_CB[!duplicated(top_Skills_CB), ]
top_Skills_CB$skills<-as.character(top_Skills_CB$skills)
top_Skills_CB[order(-top_Skills_CB$n),]
```
```{r}
tables <- list(top_Skills,top_Skills_Indeed,top_Skills_CB)
master_skills_df <- reduce(tables,left_join,by="skills")
master_skills_df
```

##Final Master Data Frame.

####The frequency reports from the three data frames were compiled into one, the master_skills_df which tallies 912 skills listed on all three sites, the frequency per site and a total of all three sites.


```{r}
master_skills_df$Total<- rowSums(master_skills_df[, c("n.x", "n.y", "n")], na.rm = TRUE)
```

```{r}
colnames(master_skills_df) <- c("Skills", "Indeed", "CareerBuilder","LinkedIn", "Total")
master_skills_df
```

```{r}
write.csv(master_skills_df, file = "Master_Skills_List.csv")
```

##II. Database RMD

In this section, we detailed how the data was compiled into a relational database.

#Overview

In order to load the data we scraped from the three different websites, we'll first clean our data. The following would need to be done:

* Unify the variable names so that each variable has only one name
* Clean the data where necessary
* Create separate data frames for each normalized table
* Load our data frames as a table into our SQLite database



#Cleaning Our Data

###LinkedIn

Let's load the LinkedIn csv file from our web scrape and take a quick glance.

```{r}
linkedin <- read.csv('LinkedIn.csv', stringsAsFactors = FALSE)
knitr::kable(head(linkedin), format = "html")
```

Looks mostly clean already. We'll add another column to identify the source of the data, which we'll also do for the other two data sets.

```{r}
linkedin <- linkedin %>%
  mutate(website = "linkedin.com")

knitr::kable(head(linkedin), format = "html")
```

###Indeed

Now we'll load the csv from the Indeed web scrape. We'll skip displaying the data from this data frame due to the large amount of text in one of our columns.

```{r}
indeed <- read.csv('Data_Science_Indeed_Jobs.csv', stringsAsFactors = FALSE)
names(indeed)
```

From viewing this data, we can tell that "X" is actually our id for each job posting. We also will not need the link column as the other two datasets do not have it. However, we will keep the "Company" column as we were unable to scrape a specific industry from Indeed. We'll clean up the column names so that it's in line with the data frame for LinkedIn. We'll also make a small edit to `job_id` in order to keep things more uniform.

```{r}
indeed <- indeed %>%
  mutate(website = "indeed.com") %>%
  rename(job_id = X,
        title = Title,
        company = Company,
        description = Description
         ) %>%
  select(-Link)
indeed$job_id <- paste("IND",as.character(indeed$job_id), sep = "")

str(indeed)
```

###CareerBuilder

Our web scrape for CareerBuilder also includes a text heavy description column, so we'll just take a look at the structure to get an idea.

```{r}
career <- read.csv('careerbuilder.csv', stringsAsFactors = FALSE)
str(career)
```

From viewing the structure of the data frame above, we'll remove the "X" column, rename the "jobdesc" and "jobbank..Job.Title." column to the appropriate names, and add the column to indicate the website we scraped from for this set. We'll also fix the job_id to be in line with the previous two data frames.

```{r}
career <- career %>%
  rename(title = jobbank..Job.Title.,
         description = jobdesc) %>%
  
  mutate(website = "careerbuilder.com")
career$job_id <- paste("CB", as.character(career$job_id), sep = "")

str(career)
```

###Merging the Three Data Frames

Now that we have cleaned and prepared our three separate data sets from the three different sources, we can now unite them into a single data frame.

```{r}
main_df <- full_join(linkedin,indeed, 
                     by=c("job_id",
                          "title",
                          "website"))
main_df <- full_join(main_df,career,
                     by=c("job_id",
                          "title",
                          "industry",
                          "description",
                          "website"))

knitr::kable(head(main_df), format = "html")
```

Now that we have our single data frame, we can move onto building our database.

#Building the Database in RSQLite

In order to create our database to conform to standards, we've created separate dataframes to identify unique values for each variable in our main dataframe. We then applied IDs for each of those unique values. Each dataframe represents a table in our database. The below code takes the unique values for each column except for the jobs_id column. We'll also check to see each dataframe to make sure it has what we want.

```{r}
industry <- unique(main_df$industry)
industry_id <- paste("IN",as.character(1:length(industry)),sep = "")
industry_df <- data.frame(industry,industry_id)
knitr::kable(head(industry_df), format = "html")

skills <- unique(main_df$skills)
skills_id <- paste("SK",as.character(1:length(skills)),sep = "")
skills_df <- data.frame(skills,skills_id)
knitr::kable(head(skills_df), format = "html")

title <- unique(main_df$title)
title_id <- paste("TI",as.character(1:length(title)),sep = "")
title_df <- data.frame(title,title_id)
knitr::kable(head(title_df), format = "html")

company <- unique(main_df$company)
company_id <- paste("CO",as.character(1:length(company)),sep = "")
company_df <- data.frame(company,company_id)
knitr::kable(head(company_df), format = "html")

website <- unique(main_df$website)
website_id <- paste("WS",as.character(1:length(website)),sep = "")
website_df <- data.frame(website,website_id)
knitr::kable(head(website_df), format = "html")

description <- unique(main_df$description)
description_id <- paste("D",as.character(1:length(description)),sep = "")
description_df <- data.frame(description,description_id)
str(description_df)
```

Now we'll build our jobs dataframe, which is a replica of the original main dataframe, but with ids instead of actual values for each variable. Doing this will allow for more flexibility and efficiency if we'd like to answer a variety of different questions.

```{r, warning=FALSE}
jobs_df <- main_df %>%
  left_join(industry_df, by="industry") %>%
  left_join(skills_df, by="skills") %>%
  left_join(title_df, by="title") %>%
  left_join(company_df, by="company") %>%
  left_join(website_df, by="website") %>%
  left_join(description_df, by="description") %>%
  select(-industry,-skills,-title,-company,-website,-description)
knitr::kable(head(jobs_df), format = "html")
```

We can now load our data into a database and create our connection.

```{r}
ds_job_db <- dbConnect(RSQLite::SQLite(), "")

dbWriteTable(ds_job_db, "industry", industry_df)
dbWriteTable(ds_job_db, "skills", skills_df)
dbWriteTable(ds_job_db, "title", title_df)
dbWriteTable(ds_job_db, "company", company_df)
dbWriteTable(ds_job_db, "website", website_df)
dbWriteTable(ds_job_db, "description", description_df)
dbWriteTable(ds_job_db, "jobs", jobs_df)
```

#Exploratory Summary Analysis

We'll now take a look at the following by writing SQL queries for each summary question and displaying on a simple graph using ggplot2:

* Total number of jobs by website
* Average number of data science skills required by each industry

###Total Number of Jobs by Website

```{r}
jobs_cnt <- dbGetQuery(ds_job_db,
           "
SELECT DISTINCT
  W.WEBSITE as Website
  ,COUNT(DISTINCT J.JOB_ID) AS 'Number of Jobs'
FROM JOBS J
  INNER JOIN WEBSITE W ON J.WEBSITE_ID = W.WEBSITE_ID
GROUP BY
  W.WEBSITE
           "
           )

ggplot(jobs_cnt, aes(Website,`Number of Jobs`)) +
  geom_bar(stat = "identity", aes(fill=Website)) +
  theme_minimal() +
  geom_text(aes(label = `Number of Jobs`),vjust = 2)
```

###Top 25 Industries that on average request the most number of skills.

```{r}
s_avg <- dbGetQuery(ds_job_db,
           "
SELECT
  MAIN.INDUSTRY
  ,AVG(MAIN.COUNT) AS 'Average Number of Skills'
FROM (
  SELECT DISTINCT
    I.INDUSTRY
    ,J.JOB_ID
    ,COUNT(DISTINCT S.SKILLS_ID) AS COUNT
  FROM JOBS J
    INNER JOIN INDUSTRY I ON J.INDUSTRY_ID = I.INDUSTRY_ID
    INNER JOIN SKILLS S ON J.SKILLS_ID = S.SKILLS_ID
  GROUP BY
    I.INDUSTRY
    ,J.JOB_ID
) MAIN
WHERE
  MAIN.INDUSTRY IS NOT NULL
GROUP BY
  MAIN.INDUSTRY
ORDER BY
  AVG(MAIN.COUNT) DESC
LIMIT 25
           "
           )
knitr::kable(s_avg, format = "html")
```

##III. Data Analysis

####When sorting by the Total number of skills, we can see an outlier for the skill, "D", which seems to be an error.  No research shows that "D" is a job skill. so, that will be removed and saved into a revised data frame. 


```{r}
master_skills_df[order(-master_skills_df$Total),]
```
```{r}
master_skills_df_01 <- master_skills_df[!master_skills_df$Skills == "D",]

```
```{r}
master_skills_df_01[order(-master_skills_df_01$Total),]
```
####Top 50 Skills
#####Next, we pared down the list to the Top 50 skills by limiting the list to those skills which have more than 52 mentions in the job postings.

```{r}

Top50_df <- master_skills_df_01[master_skills_df_01$Total>52,]

Top50_df <- Top50_df[order(-Top50_df$Total),]
Top50_df 
```





```{r}
Top50_df_percentages <- Top50_df %>% 
                        mutate(Indeed_pct = round(Indeed / sum(Indeed), 3),
                               CareerBuilder_pct = round(CareerBuilder / sum(CareerBuilder),3),
                               LinkedIn_pct = round(LinkedIn / sum(LinkedIn),3),
                               Total_pct = round(Total / sum(Total),3)) %>% 
                        select(Skills,Indeed_pct,CareerBuilder_pct,LinkedIn_pct,Total_pct) 
                        
head(Top50_df_percentages,10)

```


```{r}
tail(Top50_df_percentages,10)
```





```{r}
Top5<- Top50_df[Top50_df$Total>556,]
Top5<-Top5[order(-Top5$Total),]
Top5
```

```{r}
ggplot(data=Top5, aes(x=Skills, y=Total))+
    geom_bar(position="dodge",stat="identity",fill=c("orange")) + 
  coord_flip() +
  ggtitle("Top Five Most In Demand Data Science Skills")
```


```{r}
wordcloud(Top50_df$Skills, Top50_df$Total)
```







##Conclusions

The primary conclusion that we can draw is that the programming languages C, C++, R, Python, and SQL are mentioned the most in all of the job ads accounting for almost 64% of the total amount of skills requested in job ads which means that ALL other skills account for only 36% combined of what's requested in job ads. Also, R is more preferred than Python.  



