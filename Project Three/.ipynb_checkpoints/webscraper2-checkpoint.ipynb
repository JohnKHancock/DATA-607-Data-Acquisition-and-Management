{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='Introduction'><font color=black><u><center>Introduction - Python Web Scraping</center></u></font></a>\n",
    "The python code below was used to scrape web pages from the job website, Indeed.com. \n",
    "\n",
    "The process was as follows:\n",
    "1. Create a variable, URL, to hold a link to a search for Data Science job in New York, NY.\n",
    "2. To navigate through each page of the search, the base url needs the extension, \"&start=\" with the next page beginning at 10.\n",
    "3. Create a data frame to hold the results of the scrape.\n",
    "4. Create a for loop which navigates through each page of the search for Data Science jobs in New York, clicks on each link for each job, scrapes the Job Title, Company, Description, and Link for each job listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base Url for New York, NY Data Science Jobs\n",
    "URL = \"https://www.indeed.com/jobs?q=Data+Science&l=New+York%2C+NY\"\n",
    "Base_URL = \"https://www.indeed.com/jobs?q=Data+Science&l=New+York%2C+NY\"\n",
    "pages = '&start='\n",
    "counter = 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Python data frame to hold the results\n",
    "JobsB_df = pd.DataFrame(columns=['Title', 'Company', 'Description', 'Link'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For loop that navigates through the pages of the search result\n",
    "for i in range(1,1000):\n",
    "#Automates a FireFox web browser which searches and open links\n",
    "    driver = webdriver.Firefox()\n",
    "    print(URL)\n",
    "    driver.get(URL)\n",
    "    driver.implicitly_wait(20)\n",
    "#Creates a list of links for each job listed on each page of the search.  Identifies the links by searching for XPath element for each job.\n",
    "    elements = driver.find_elements_by_xpath('//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"jobtitle\", \" \" ))]//*[contains(concat( \" \", @class, \" \" ), concat( \" \", \"turnstileLink\", \" \" ))]')\n",
    "#Another for loop that loops through the elements list, clicks on each job link.   \n",
    "    for job in elements:\n",
    "        try:\n",
    "            job.click()\n",
    "#Scrapes the page by locating the XPath for the job description, title, company name and then appends results to the pandas data frame\n",
    "            post = driver.find_element_by_xpath('//*[@id=\"vjs-desc\"]').text\n",
    "            title = driver.find_element_by_xpath('//*[@id=\"vjs-jobtitle\"]').text \n",
    "            company = driver.find_element_by_xpath('//*[@id=\"vjs-cn\"]/a').text             \n",
    "            JobsB_df = JobsB_df.append({'Title':title,'Company':company, 'Description':post, 'Link':URL}, ignore_index=True)\n",
    "        except:\n",
    "            pass\n",
    "#closes the browser        \n",
    "    driver.quit()\n",
    "#Increments the counter by one to move to the next page of the search and then appends to the base url which moves on to the next page.\n",
    "    counter = counter + i\n",
    "    pages = '&start=' + str(counter)\n",
    "    URL = Base_URL + pages   \n",
    "    counter = 9\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JobsB_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminates duplicate job postings\n",
    "Final_Jobs_df=JobsB_df.drop_duplicates(['Company'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writes the results to a csv file.\n",
    "Final_Jobs_df.to_csv(\"Indeed Data Science Jobs Revised_01.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
